{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align: center;\" markdown=\"1\">Machine Learning Algorithms for Poverty Prediction</h1> \n",
    "<h2 style=\"text-align: center;\" markdown=\"2\">A project of the World Bank's Knowledge for Change Program</h2>\n",
    "<h3 style=\"text-align: center;\" markdown=\"3\">(KCP, Grant TF0A4534)</h3>\n",
    "\n",
    "\n",
    "> *This notebook is part of a series that has been developed as an empirical comparative assessment of machine learning classification algorithms applied to poverty prediction. The objectives of this project are to explore how well machine learning algorithms perform when given the task to identify the poor in a given population, and to provide a resource of machine learning techniques for researchers, data scientists, and statisticians in developing countries.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Additional Years of Indonesia Surveys <a class=\"anchor\" id=\"description\"></a>\n",
    "\n",
    "In this notebook, we prepare additional data sets for testing algorithms towards the end of the project. The preparation process is exactly the same, except that this time we don't need any training data (the algortihms are already trained)\n",
    "\n",
    "This data prepares Indonesia data from three new years,\n",
    "\n",
    "* 2011\n",
    "* 2013\n",
    "* 2014\n",
    "\n",
    "As mentioned above, the process is the same. The only exception is that new data is compared against the original 2012 data such that the columns (questions) of all data sets are equal. For a few columns, this required mapping different names to the original data set, which we point out below.\n",
    "\n",
    "### Raw data overview\n",
    "The data for this project includes data and metadata from surveys in Indonesia for the years 2011,m 2013, 2014. For each year, you will find:\n",
    "* The survey questionnaire(s)\n",
    "* Two Stata (version 14) data files: household and individual. These data contain a subset of the full survey dataset\n",
    "* An Excel file, providing a list of variables in the data files\n",
    "* Two text files (output of the “codebook” Stata command, providing a list of variables with some metadata.\n",
    " \n",
    "### Stata file contents\n",
    "The Stata data files contain:\n",
    "* The label variable (the binary variable to be predicted), named “poor”. This is just a poor/non-poor dummy variable. Note: poverty is calculated at the household level; either all or none of one same household member is/are poor.\n",
    "* The sample weights (the objective is to “predict” poverty taking into account these sample weights. Variable `wta_hh` contains the household weight; variable `wta_pop` contains the population weight (household weight * household size). These are probability weights (the sum of `wta_pop` gives us the extrapolated population of the country).\n",
    "* The file that will be used as training/test set is the `household.dta` file.\n",
    "* The `individual.dta` file contains variables on each household member. The file is provided to allow us to create additional variables to be added to the `household.dta` file. The variable `hhsize` (household size) is one such derived variable, already calculated. We will probably use this to generate new variables to possibly improve the accuracy of the model.\n",
    "\n",
    "### Filtering Consumable Feaures\n",
    "The `household.dta` file contains a series of variables whose name starts with `cons_`. These are variables derived from the expenditure module of the survey. It indicates whether a household consumed/purchased a specific item or not. Any reported value > 0 for an item was transformed into code 1; 0 otherwise.  \n",
    "\n",
    "*IMPORTANT*: Since one objective of this project is to design light survey questionnaires that would allow accurate poverty prediction, we want to be selective in the list of variables we use in the final model. What we would like to do is optimize a model using the variables we find useful (including new derived variables), under the constraint that no more than 50 of the `cons_` variables will be used. Some quick analysis of these `cons_` variables should make it possible to make a selection prior to tuning the models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Raw Data <a class=\"anchor\" id=\"load-data\"></a>\n",
    "\n",
    "First, we load a few essential modules used in notebook. We have developed several utility functions in the `load_data.py` file located in the `src/data` directory that will be used throughout this project for convenience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas.io.stata import StataReader\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython.display import display\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Add our local functions to the path\n",
    "sys.path.append(os.path.join(os.pardir, 'src'))\n",
    "from data import load_data\n",
    "from features import process_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load with same function as before <a class=\"anchor\" id=\"load\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_stata_file(filepath, \n",
    "                    index_cols, \n",
    "                    update_education=False, \n",
    "                    drop_minornans=False, \n",
    "                    drop_unlabeled=False):\n",
    "    \"\"\" Load data and metadata from Stata file\"\"\"\n",
    "    data = pd.read_stata(filepath, convert_categoricals=False).set_index(index_cols)\n",
    "\n",
    "    with StataReader(filepath) as reader:\n",
    "        reader.value_labels()\n",
    "            \n",
    "        mapping = {col: reader.value_label_dict[t] for col, t in \n",
    "                   zip(reader.varlist, reader.lbllist)\n",
    "                   if t in reader.value_label_dict}\n",
    "        \n",
    "        # manually update some specific columns\n",
    "        # in the Malawi dataset\n",
    "        if update_education and 'ind_educ09' in mapping:\n",
    "            mapping['ind_educ09'][13] = \"Primary - \" + mapping['ind_educ09'][13]\n",
    "            mapping['ind_educ09'][14] = \"Primary - \" + mapping['ind_educ09'][14]\n",
    "            \n",
    "            mapping['ind_educ09'][23] = \"Secondary - \" + mapping['ind_educ09'][23]\n",
    "            mapping['ind_educ09'][24] = \"Secondary - \" + mapping['ind_educ09'][24]\n",
    "        \n",
    "        data.replace(mapping, inplace=True)\n",
    "        \n",
    "        # convert the categorical variables into\n",
    "        # the category type\n",
    "        for c in data.columns:\n",
    "            if c in mapping:\n",
    "                data[c] = data[c].astype('category')\n",
    "                        \n",
    "        # drop records with only a few nans\n",
    "        if drop_minornans: \n",
    "            nan_counts = (data.applymap(pd.isnull)\n",
    "                          .sum(axis=0)\n",
    "                          .sort_values(ascending=False))\n",
    "            nan_cols = nan_counts[(nan_counts > 0) & (nan_counts < 10)].index.values\n",
    "            data = data.dropna(subset=nan_cols)\n",
    "        # drop unlabeled categorical values\n",
    "        def find_unlabeled(x):\n",
    "            if x.name in mapping.keys():\n",
    "                return [val if (val in mapping[x.name].values() or pd.isnull(val)) \n",
    "                        else 'UNLABELED' for val in x]\n",
    "            else:\n",
    "                return x\n",
    "            \n",
    "        data = data.apply(find_unlabeled)\n",
    "        data = data[~data.applymap(lambda x: x == \"UNLABELED\").any(axis=1)]\n",
    "        \n",
    "        # read the actual questions that were asked for reference\n",
    "        questions = reader.variable_labels()\n",
    "        \n",
    "    return data, questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Here is a simple function to help us find mismatched columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from difflib import get_close_matches\n",
    "\n",
    "def find_mismatched_columns(new, old):\n",
    "    in_new_but_not_old = set(new.columns.values) - set(old.columns.values)\n",
    "\n",
    "    # don't want to kill the weights and labels, they are loaded sep so won't appear here\n",
    "    for col in ['wta_pop', 'wta_hh', 'poor']:\n",
    "        in_new_but_not_old.discard(col)\n",
    "\n",
    "    in_old_but_not_new = set(old.columns.values) - set(new.columns.values)\n",
    "    return in_new_but_not_old, in_old_but_not_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here is a map between column names to that all features are the same for 2011-2014"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# manually created column map               \n",
    "col_name_map = {\n",
    "'hld_bwwater__Pump': 'hld_bwwater__Pipe with meter',\n",
    "'hld_bwwater__Retail piping water': 'hld_bwwater__Pipe, retail payment',\n",
    "'hld_bwwater__Others': 'hld_bwwater__Other',\n",
    "'hld_dwater__Recycled bottled/ refill water': 'hld_dwater__Recycled bottled water',\n",
    "'hld_wall__Concrete/brick': 'hld_wall__Concrete',\n",
    "'hld_bwwater__Protected well': 'hld_bwwater__Protected/covered well',\n",
    "'hld_toilet__Pit latrine': 'hld_toilet__Pit toilet/plengsengan',\n",
    "'hld_dwater__Meter piping meter': 'hld_dwater__Pipe with meter',\n",
    "'hld_bwwater__Unprotected well': 'hld_bwwater__Unprotected/uncovered well',\n",
    "'hld_bwwater__Rain water': 'hld_bwwater__Rainwater',\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We now load and process the new years: 2011, 2013, 2014\n",
    "\n",
    "Along the way, we'll keep track of mismatched columns (printed in the cell output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t---- 2011 ----\n",
      "Indonesia household data has 71,932 rows and 359 columns\n",
      "Percent poor: 11.1% \tPercent non-poor: 88.9%\n",
      "Indonesia individual data has 284,539 rows and 53 columns\n",
      "Percent poor: 13.8% \tPercent non-poor: 86.2%\n",
      "Indonesia household with dummy variables added (71932, 814)\n",
      "Indonesia household with constant columns dropped (71932, 463)\n",
      "Indonesia household shape with duplicate columns dropped (71932, 455)\n",
      "Reducing to 453 features...\n",
      "\n",
      "\n",
      "After rename, in new data but not old:\t{'hld_cooking__Never cooking', 'hld_toilet__Water seal latrine'}\n",
      "dropping...\n",
      "In old data but not new:\t{'hld_toilet__Squat toilet/cemplung', 'hld_bwwater__Recycled bottled water', 'hld_bwwater__Terrestrial well/pump'}\n",
      "filling...\n",
      "\n",
      "\n",
      "\t\t---- 2013 ----\n",
      "Indonesia household data has 70,842 rows and 356 columns\n",
      "Percent poor: 7.7% \tPercent non-poor: 92.3%\n",
      "Indonesia individual data has 274,051 rows and 55 columns\n",
      "Percent poor: 9.8% \tPercent non-poor: 90.2%\n",
      "Indonesia household with dummy variables added (70842, 811)\n",
      "Indonesia household with constant columns dropped (70842, 460)\n",
      "Indonesia household shape with duplicate columns dropped (70842, 453)\n",
      "Reducing to 453 features...\n",
      "\n",
      "\n",
      "After rename, in new data but not old:\t{'hld_cooking__Never cooking', 'hld_cope1__Not applicable', 'hld_toilet__Water seal latrine'}\n",
      "dropping...\n",
      "In old data but not new:\t{'hld_toilet__Squat toilet/cemplung', 'hld_bwwater__Recycled bottled water', 'hld_bwwater__Terrestrial well/pump', 'geo_district', 'geo_village', 'geo_subdistrict'}\n",
      "filling...\n",
      "\n",
      "\n",
      "\t\t---- 2014 ----\n",
      "Indonesia household data has 70,590 rows and 357 columns\n",
      "Percent poor: 6.6% \tPercent non-poor: 93.4%\n",
      "Indonesia individual data has 272,636 rows and 53 columns\n",
      "Percent poor: 8.5% \tPercent non-poor: 91.5%\n",
      "Indonesia household with dummy variables added (70590, 812)\n",
      "Indonesia household with constant columns dropped (70590, 461)\n",
      "Indonesia household shape with duplicate columns dropped (70590, 453)\n",
      "Reducing to 453 features...\n",
      "\n",
      "\n",
      "After rename, in new data but not old:\t{'hld_cooking__Never cooking', 'hld_toilet__Water seal latrine'}\n",
      "dropping...\n",
      "In old data but not new:\t{'hld_toilet__Squat toilet/cemplung', 'hld_bwwater__Recycled bottled water', 'hld_bwwater__Terrestrial well/pump', 'geo_village', 'geo_subdistrict'}\n",
      "filling...\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# for tracking mismatches below\n",
    "mismatches = {}\n",
    "\n",
    "for year in [2011, 2013, 2014]:\n",
    "    \n",
    "    print(f'\\t\\t---- {year} ----')\n",
    "\n",
    "    # Load Indonesia household data\n",
    "    filepath = eval(f'load_data.IDN_{year}_HOUSEHOLD')\n",
    "    idn_hhold, idn_hhold_questions = load_stata_file(filepath, ['hid'])\n",
    "    s = 'Indonesia household data has {:,} rows and {:,} columns'\n",
    "    print(s.format(idn_hhold.shape[0], idn_hhold.shape[1]))\n",
    "    s = 'Percent poor: {:0.1%} \\tPercent non-poor: {:0.1%}'\n",
    "    print(s.format(idn_hhold.poor.value_counts(normalize=True).loc['Poor'], \n",
    "                   idn_hhold.poor.value_counts(normalize=True).loc['Non-poor']))\n",
    "    idn_hhold.head()\n",
    "\n",
    "    # Load Indonesia individual data\n",
    "    filepath = eval(f'load_data.IDN_{year}_INDIVIDUAL')\n",
    "    idn_indiv, idn_indiv_questions = load_stata_file(filepath, ['hid', 'iid'])\n",
    "    s = 'Indonesia individual data has {:,} rows and {:,} columns'\n",
    "    print(s.format(idn_indiv.shape[0], idn_indiv.shape[1]))\n",
    "    s = 'Percent poor: {:0.1%} \\tPercent non-poor: {:0.1%}'\n",
    "    print(s.format(idn_indiv.poor.value_counts(normalize=True).loc['Poor'], \n",
    "                   idn_indiv.poor.value_counts(normalize=True).loc['Non-poor']))\n",
    "\n",
    "    idn_hhold.poor = (idn_hhold.poor == 'Poor')\n",
    "\n",
    "    def add_derived_feature(df, \n",
    "                            feature_name, \n",
    "                            values, \n",
    "                            questions_dict, \n",
    "                            question=None):\n",
    "        '''Add a derived feature to the household dataframe and update questions'''\n",
    "        df['der_' + feature_name] = values\n",
    "        questions_dict['der_' + feature_name] = question\n",
    "        return\n",
    "\n",
    "    # IDN - Number of children (10 and under) in household\n",
    "    values = (idn_indiv.ind_age <= 10).sum(level=0).astype(int)\n",
    "    question = 'Number of children in household age 10 and under'\n",
    "    add_derived_feature(idn_hhold, \n",
    "                        'nchild10under', \n",
    "                        values, \n",
    "                        idn_hhold_questions, \n",
    "                        question=question)\n",
    "\n",
    "    # IDN - Number of males and females in household over age 10\n",
    "    values = ((idn_indiv.ind_sex == 'Male') & (idn_indiv.ind_age > 10)).sum(level=0).astype(int)\n",
    "    question = 'Number of males in household over age 10'\n",
    "    add_derived_feature(idn_hhold, \n",
    "                        'nmalesover10', \n",
    "                        values, \n",
    "                        idn_hhold_questions, \n",
    "                        question=question)\n",
    "    values = ((idn_indiv.ind_sex == 'Female') & (idn_indiv.ind_age > 10)).sum(level=0).astype(int)\n",
    "    question = 'Number of females in household over age 10'\n",
    "    add_derived_feature(idn_hhold, \n",
    "                        'nfemalesover10', \n",
    "                        values, \n",
    "                        idn_hhold_questions, \n",
    "                        question=question)\n",
    "\n",
    "    # IDN - Number of household members who can read and write in any language\n",
    "    values = (idn_indiv.ind_educ11 == 'Yes').sum(level=0).astype(int)\n",
    "    question = 'Number of household members who can read and write'\n",
    "    add_derived_feature(idn_hhold, \n",
    "                        'nliterate', \n",
    "                        values, \n",
    "                        idn_hhold_questions, \n",
    "                        question=question)\n",
    "\n",
    "    # IDN - Number of household members employed in past 3 months\n",
    "    values = (idn_indiv.ind_work3 == 'Yes').sum(level=0).astype(int)\n",
    "    question = 'Number of household members employed in past 3 months (over age 10)'\n",
    "    add_derived_feature(idn_hhold, \n",
    "                        'nemployedpast3mo',\n",
    "                        values,\n",
    "                        idn_hhold_questions,\n",
    "                        question=question)\n",
    "\n",
    "    # IDN - Number of household members who accessed the internet in past 3 months\n",
    "    values = (idn_indiv.ind_educ15 == 'Yes').sum(level=0).astype(int)\n",
    "    question = 'Number of household members who accessed the internet in past 3 months'\n",
    "    add_derived_feature(idn_hhold, \n",
    "                        'ninternetpast3mo',\n",
    "                        values,\n",
    "                        idn_hhold_questions,\n",
    "                        question=question)\n",
    "\n",
    "    # create dummy variables for categoricals\n",
    "    idn_hhold = pd.get_dummies(idn_hhold, drop_first=True, dummy_na=True, prefix_sep='__')\n",
    "\n",
    "    print(\"Indonesia household with dummy variables added\", idn_hhold.shape)\n",
    "\n",
    "    # remove columns with only one unique value (all nan dummies from columns with no missing values)\n",
    "    idn_hhold = idn_hhold.loc[:, idn_hhold.nunique(axis=0) > 1]\n",
    "\n",
    "    print(\"Indonesia household with constant columns dropped\", idn_hhold.shape)\n",
    "\n",
    "    # remove duplicate columns - these end up being all from nan or Not Applicable dummies \n",
    "    process_features.drop_duplicate_columns(idn_hhold, ignore=['wta_hh', 'wta_pop'], inplace=True)\n",
    "\n",
    "    print(\"Indonesia household shape with duplicate columns dropped\", idn_hhold.shape)\n",
    "\n",
    "    # it's all test data, just need test path\n",
    "    _, TEST_PATH, QUESTIONS_PATH = load_data.get_country_filepaths(f'idn-{year}')\n",
    "    \n",
    "    # select out columns not in trained model, zero-fill columns not in new data, try to match names where possible\n",
    "    _, TEST_PATH_TO_MATCH, QUESTIONS_PATH_TO_MATCH = load_data.get_country_filepaths('idn')\n",
    "    test_to_match, _, _ = load_data.load_data(TEST_PATH_TO_MATCH)\n",
    "    \n",
    "    print(f'Reducing to {test_to_match.columns.values.shape[0]} features...\\n')\n",
    "    \n",
    "    idn_hhold.rename(col_name_map, axis=1, inplace=True)   \n",
    "    \n",
    "    in_new_but_not_old, in_old_but_not_new = find_mismatched_columns(idn_hhold, test_to_match)\n",
    "    \n",
    "    close_matches = {new_col: get_close_matches(new_col, in_old_but_not_new) for new_col in in_new_but_not_old}\n",
    "    mismatches[year] = close_matches\n",
    "\n",
    "    # drop any remaining mismatches\n",
    "    in_new_but_not_old, in_old_but_not_new = find_mismatched_columns(idn_hhold, test_to_match)\n",
    "    \n",
    "    print(f'\\nAfter rename, in new data but not old:\\t{in_new_but_not_old}')\n",
    "    print('dropping...')\n",
    "    idn_hhold.drop(in_new_but_not_old, inplace=True, axis=1)\n",
    "    \n",
    "    print(f'In old data but not new:\\t{in_old_but_not_new}')\n",
    "    print('filling...\\n\\n')\n",
    "    \n",
    "    for col in in_old_but_not_new:\n",
    "        idn_hhold[col] = 0\n",
    "        idn_hhold[col] = 0\n",
    "    \n",
    "\n",
    "    idn_hhold.to_pickle(TEST_PATH)\n",
    "    with open(QUESTIONS_PATH, 'w') as fp:\n",
    "        json.dump(idn_hhold_questions, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display the mismatches for each year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "**************************\n",
      "\n",
      "2011\n",
      "**************************\n",
      "\n",
      "hld_cooking__Never cooking:\n",
      "\t\tNO MATCHES FOUND (with >= .6 probability)\n",
      "hld_toilet__Water seal latrine:\n",
      "\t\tNO MATCHES FOUND (with >= .6 probability)\n",
      "\n",
      "**************************\n",
      "\n",
      "2013\n",
      "**************************\n",
      "\n",
      "hld_cooking__Never cooking:\n",
      "\t\tNO MATCHES FOUND (with >= .6 probability)\n",
      "hld_cope1__Not applicable:\n",
      "\t\tNO MATCHES FOUND (with >= .6 probability)\n",
      "hld_toilet__Water seal latrine:\n",
      "\t\tNO MATCHES FOUND (with >= .6 probability)\n",
      "\n",
      "**************************\n",
      "\n",
      "2014\n",
      "**************************\n",
      "\n",
      "hld_cooking__Never cooking:\n",
      "\t\tNO MATCHES FOUND (with >= .6 probability)\n",
      "hld_toilet__Water seal latrine:\n",
      "\t\tNO MATCHES FOUND (with >= .6 probability)\n"
     ]
    }
   ],
   "source": [
    "for year, mismatch in mismatches.items():\n",
    "    print('\\n**************************\\n')\n",
    "    print(f'{year}')\n",
    "    print('**************************\\n')\n",
    "    for new_col, matches in mismatch.items():\n",
    "        print(f'{new_col}:')\n",
    "        if not matches:\n",
    "            print(f'\\t\\tNO MATCHES FOUND (with >= .6 probability)')\n",
    "        for match in matches:\n",
    "            print(f'\\t\\t{match}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data are now ready for prediction!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
